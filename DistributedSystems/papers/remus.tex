
Remus: High Availability via Asynchronous Virtual Machine Replication

Abstract

Allowing applications to survive hardware failure is an expensive undertaking,
which generally involves reengineering software to include complicated
recovery logic as well as deploying special-purpose hardware; this represents a
severe barrier to improving the dependability of large or legacy applications.
%Allowing applications to survive hardware failure is an expensive undertaking, 
%which generally involves  
We describe the construction of a general and transparent high
availabilityservice that allows existing, unmodified software to be protected
from the failure of the physical machine on which it runs. 
Remus provides an extremely high degree of fault tolerance, to the point that a
running system can transparently continue execution on an alternate physical
host in the face of failure with only seconds of downtime, while completely
preserving host state such as active network connections. 
Our approach encapsulates protected software in a virtual machine,
asynchronously propagates changed state to a backup host at frequencies as high
as forty times a second, and uses speculative execution to concurrently run
the active VM slightly ahead of the replicated system state.

1 Introduction

Highly available systems are the purview of the very richand the very scared.
However, the desire for reliability is pervasive, even among system designers
with modest resources.

Unfortunately, high availability is hard -- it requires that systems be
constructed with redundant components that are capable of maintaining and
switching to back-ups in the face of failure. 
Commercial high availability systems that aim to protect modern servers
generally use specialized hardware, customized software, or both (e.g [12]). 
In each case, the ability to transparently survive failure is complex and
expensive enough to prohibit deployment on common servers.

This paper describes Remus, a software system that provides OS and
application-agnostic high availability on commodity hardware. 
Our approach capitalizes on the ability of virtualization to migrate running VMs
between physical hosts, and extends the technique toreplicate snapshots of
an entire running OS instance at very high frequencies -- as often as every
25ms -- between a pair of physical machines. 
Using this technique, our system discretizes the execution of a VM into a
series of replicated snapshots. 
External output, specifically transmitted network packets, is not released
until the sys-tem state that produced it has been replicated.

Virtualization makes it possible to create a copy of arunning machine, but it
does not guarantee that the process will be efficient. 
Propagating state synchronouslyat every change is impractical: it effectively
reduces the throughput of memory to that of the network device per-forming
replication. 
Rather than running two hosts in lock-step [4] we allow a single host to
execute speculatively and then checkpoint and replicate its state
asyn-chronously. 
System state is not made externally visible until the checkpoint is committed --
we achieve high-speed replicated performance by effectively running the system
tens of milliseconds in the past.

The contribution of this paper is a practical one.
Whole-system replication is a well-known approach to providing high
availability.
However, it usually has been considered to be significantly more expensive than
application-specific checkpointing techniques that only replicate relevant data
[15]. 
Our approach may be used to bring HA "to the masses" as a platform service
forvirtual machines. 
In spite of the hardware and software constraints under which it operates, this
system providesprotection equal to or better than expensive commercial
offerings. 
Many existing systems only actively mirrorpersistent storage, requiring
applications to perform recovery from crash-consistent persistent state. 
In contrast, Remus ensures that regardless of the moment at which the primary
fails, no externally visible state is ever lost.

1.1 Goals
Remus aims to make mission-critical availability acces-sible to mid- and
low-end systems. 
By simplifying provisioning and allowing multiple servers to be consolidatedon
a smaller number of physical hosts, virtualization has made these systems more
popular than ever. 
However,the benefits of consolidation come with a hidden cost in the form of
increased exposure to hardware failure.
Remus addresses this by commodifying high availability as a service offered by
the virtualization platform itself,providing administrators of individual VMs
with a tool to mitigate the risks associated with virtualization.

Remus's design is based on the following high-levelgoals:

Generality. It can be prohibitively expensive to customize a single application
to support high availability, let alone the diverse range of software upon
which an organization may rely. 
To address this issue, high availability should be provided as a low-level
service, with common mechanisms that apply regardless of the application being
protected or the hardware on which it runs.Transparency.

The reality in many environments isthat OS and application source may not even be available to modify. To support the broadest possible range ofapplications with the smallest possible barrier to entry,
high availability should not require that OS or applica-tion code be modified to support facilities such as failure
detection or state recovery.Seamless failure recovery.

No externally visible stateshould ever be lost in the case of single-host failure.

Furthermore, failure recovery should proceed rapidlyenough that it appears as nothing more than temporary
packet loss from the perspective of external users. Estab-lished TCP connections should not be lost or reset.

These are lofty goals, entailing a degree of protec-tion well beyond that provided by common HA systems,
which are based on asynchronous storage mirroring fol-lowed by application-specific recovery code. Moreover,
the desire to implement this level of availability without modifying the code within a VM necessitates a verycoarse-grained approach to the problem. A final and pervasive goal of the system is that it realize these goalswhile providing deployable levels of performance even
in the face of SMP hardware that is common on today'sserver hardware.

1.2 Approach
Remus runs paired servers in an active-passive configura-tion. We employ three major techniques in order to overcome the difficulties traditionally associated with this ap-proach. First, we base our system on a virtualized infrastructure to facilitate whole-system replication. Second,

Figure 1: Speculative execution and asynchronous repli-cation in Remus.
we increase system performance through speculative ex-ecution, which decouples external output from synchronization points. This allows the primary server to re-main productive, while synchronization with the replicated server is performed asynchronously. The basicstages of operation in Remus are given in Figure 1.

VM-based whole-system replication. Hypervisorshave been used to build HA systems before [4]. In that
work, virtualization is used to run a pair of systems inlock-step, and additional support has been added to ensure that VMs on a pair of physical hosts follow a deter-ministic path of execution: external events are carefully
injected into both the primary and fallback VMs so thatthey result in identical states. Enforcing such deterministic execution suffers from two fundamental problems.First, it is highly architecture-specific, requiring that the
system have a comprehensive understanding of the in-struction set being executed and the sources of external
events. Second, it results in an unacceptable overheadwhen applied in multi-processor systems, where sharedmemory communication between processors must be ac-curately tracked and propagated [8].

Speculative execution. Replication may be achievedeither by copying the state of a system or by replaying
input deterministically. We believe the latter to be im-practical for real-time operation, especially in a multiprocessor environment. Therefore, Remus does not at-tempt to make computation deterministic -- there is a
very real possibility that the output produced by a systemafter a given checkpoint will be different if the system is
rolled back to that checkpoint and its input is replayed.However, the state of the replica needs to be synchronized with the primary only when the output of the pri-mary has become externally visible. Instead of letting
the normal output stream dictate when synchronizationmust occur, we can buffer output1 until a more convenient time, performing computation speculatively aheadof synchronization points. This allows a favorable tradeoff to be made between output latency and runtime overhead, the degree of which may be controlled by the ad-ministrator.

Asynchronous replication. Buffering output at theprimary server allows replication to be performed asynchronously. The primary host can resume execution atthe moment its machine state has been captured, without
waiting for acknowledgment from the remote end. Over-lapping normal execution with the replication process
yields substantial performance benefits. This enables ef-ficient operation even when checkpointing at intervals on
the order of tens of milliseconds.

2 Design and Implementation
Figure 2 shows a high-level view of our system. We be-gin by encapsulating the machine to be protected within
a VM. Our implementation is based on the Xen virtualmachine monitor [2], and extends Xen's support for live
migration to provide fine-grained checkpoints. An initialsubset of our checkpointing support has been accepted
into the upstream Xen source.Remus achieves high availability by propagating frequent checkpoints of an active VM to a backup physi-cal host. On the backup, the VM image is resident in
memory and may begin execution immediately if fail-ure of the active system is detected. Because the backup
is only periodically consistent with the primary, all net-work output must be buffered until state is synchronized
on the backup. When a complete, consistent image ofthe host has been received, this buffer is released to external clients. The checkpoint, buffer, and release cyclehappens very frequently - we include benchmark results
at frequencies up to forty times per second, representinga whole-machine checkpoint including network and ondisk state every 25 milliseconds.Unlike transmitted network traffic, disk state is not externally visible. It must, however, be propagated to theremote host as part of a complete and consistent snapshot. To maintain disk replication, all writes to the pri-mary disk are transmitted asynchronously to the backup,
where they are buffered in RAM until the correspondingmemory checkpoint has arrived. At that point, the complete checkpoint is acknowledged to the primary, whichthen releases outbound network traffic, and the buffered
disk writes are applied to the backup disk.It is worth emphasizing that the virtual machine does
not actually execute on the backup host until a failure oc-curs. It simply acts as a receptacle for checkpoints of the
active VM. This consumes a relatively small amount ofthe backup host's resources, allowing it to concurrently
protect VMs running on multiple physical hosts in an N-to-1-style configuration. Such a configuration gives administrators a high degree of freedom to balance the de-gree of redundancy against resource costs.

2.1 Failure model
Remus provides the following properties:

1. The fail-stop failure of any single host is tolerable.
2. Should both the primary and backup hosts fail con-currently, the protected system's data will be left in

a crash-consistent state.
3. No output will be made externally visible until theassociated system state has been committed to the

replica.
Our goal is to provide completely transparent recov-ery from fail-stop failures of a single physical host. The

compelling aspect of this system is that high availabilitymay be easily retrofitted onto existing software running
on commodity hardware. It uses a pair of commodityhost machines, connected over redundant gigabit Ethernet connections, and survives the failure of any one ofthese components. By incorporating block devices into
its state replication protocol, it avoids requiring expen-sive, shared network-attached storage for disk images.

We do not aim to recover from software errors or non-fail-stop conditions. As observed in [5], this class of approach provides complete system state capture and repli-cation, and as such will propagate application errors to
the backup. This is a necessary consequence of provid-ing both transparency and generality.

Our failure model is identical to that of commercialHA products, which provide protection for virtual machines today [31, 30]. However, the degree of protectionoffered by these products is substantially less than that
provided by Remus: existing commercial products re-spond to the failure of a physical host by simply rebooting the VM on another host from its crash-consistent diskstate. Our approach survives failure on time frames similar to those of live migration, and leaves the VM runningand network connections intact. Exposed state is not lost
and disks are not corrupted.

2.2 Pipelined Checkpoints
Checkpointing a running virtual machine many times persecond places extreme demands on the host system. Remus addresses this by aggressively pipelining the check-point operation. We use an epoch-based system in which
execution of the active VM is bounded by brief pauses inexecution in which changed state is atomically captured,
and external output is released when that state has beenpropagated to the backup. Referring back to Figure 1,
this procedure can be divided into four stages: (1) Onceper epoch, pause the running VM and copy any changed
state into a buffer. This process is effectively the stop-and-copy stage of live migration [6], but as described

V\Gamma \Gamma 
Pro\Delta e\Theta \Delta e\Lambda  V\Gamma 

Act\Xi \Pi \Sigma  \Upsilon \Phi \Psi t

(O\Omega fffiflffi
ffli\Omega j`fi '^*\Omega *

Re_*,\Theta ss\Delta ,oae

Eaeoe,aee

Mo/AEOEO/y

!"o/O/n#$

Do/%&'o/)

o/!"o/O/n#$
no/"wOEO/k

V\Gamma \Gamma 

Bss\Theta *u_ V\Gamma 

+,c-./ \Upsilon \Phi \Psi t

Re_*,\Theta ss\Delta ,oae

Server

Mo/AEOEO/y

0"OEO/#ao/

Ho/#O/"1o/#" Ho/#O/"1o/#"

233

45678978:23 ;8!=?9?7?6@C@F?@8

GIJKLNQTUILWXYZI["]I^.IXLU`IXU

233

45678978:23 ;8!=?9?7?6@C@F?@8

GIJKLNQTUILWXYZI["]I^.IXLU`IXU

Figure 2: Remus: High-Level Architecture
later in this section it has been dramatically optimizedfor high-frequency checkpoints. With state changes preserved in a buffer, the VM is unpaused and speculativeexecution resumes. (2) Buffered state is transmitted and
stored in memory on the backup host. (3) Once the com-plete set of state has been received, the checkpoint is acknowledged to the primary. Finally, (4) buffered networkoutput is released.

The result of this approach is that execution is effec-tively discretized at checkpoint boundaries; the acknowledgment of a completed checkpoint by the backup trig-gers the release of network traffic that has been buffered
and represents an atomic transition into the new epoch.

2.3 Memory and CPU
Checkpointing is implemented above Xen's existing ma-chinery for performing live migration [6]. Live migration

is a technique by which a virtual machine is relocatedto another physical host with only slight interruption in
service. To accomplish this, memory is copied to thenew location while the VM continues to run at the old
location. During migration, writes to memory are inter-cepted, and dirtied pages are copied to the new location
in rounds. After a specified number of intervals, or whenno forward progress is being made because the virtual
machine is writing to memory at least as fast as the mi-gration process can copy it out, the guest is suspended
and the remaining dirty memory is copied out along withthe current CPU state. At this point the image on the
new location is activated. Total downtime depends onthe amount of memory remaining to be copied when the
guest is suspended, but is typically under 100ms. Totalmigration time is a function of the amount of memory in
use by the guest, and its writable working set [6], which

is the set of pages changed repeatedly during guest exe-cution.

Xen provides the ability to track guest writes to mem-ory using a mechanism called shadow page tables. When
this mode of operation is enabled, the VMM maintains aprivate ("shadow") version of the guest's page tables and
exposes these to the hardware MMU. Page protection isused to trap guest access to its internal version of page
tables, allowing the hypervisor to track updates, whichare propagated to the shadow versions as appropriate.

For live migration, this technique is extended to trans-parently (to the guest) mark all VM memory as read only.
The hypervisor is then able to trap all writes that a VMmakes to memory and maintain a map of pages that have
been dirtied since the previous round. Each round, themigration process atomically reads and resets this map,
and the iterative migration process involves chasing dirtypages until progress can no longer be made. As mentioned above, the live migration process eventually sus-pends execution of the VM and enters a final "stop-andcopy" round, where any remaining pages are transmittedand execution resumes on the destination host.

Remus implements checkpointing as repeated execu-tions of the final stage of live migration: each epoch, the
guest is paused while changed memory and CPU stateis copied to a buffer. The guest then resumes execution
on the current host, rather than on the destination. Sev-eral modifications to the migration process are required
in order to provide sufficient performance and to ensurethat a consistent image is always available at the remote
location. These are described below.

Migration enhancements. In live migration, guestmemory is iteratively copied over a number of rounds

and may consume minutes of execution time; the briefservice interruption caused by the singular stop-and-copy

phase is not a significant overhead. This is not the casewhen capturing frequent VM checkpoints:

every check-point is just the final stop-and-copy phase of migration,

and so this represents a critical point of optimizationin reducing checkpoint overheads. An examination of
Xen's checkpoint code revealed that the majority of thetime spent while the guest is in the suspended state is lost
to scheduling, largely due to inefficiencies in the imple-mentation of the xenstore daemon that provides administrative communication between guest virtual machinesand domain 0.

Remus optimizes checkpoint signaling in two ways:First, it reduces the number of inter-process requests required to suspend and resume the guest domain. Second,it entirely removes xenstore from the suspend/resume
process. In the original code, when the migration processdesired to suspend a VM it sent a message to xend, the
VM management daemon. Xend in turn wrote a messageto xenstore, which alerted the guest by an event channel
(a virtual interrupt) that it should suspend execution. Theguest's final act before suspending was to make a hypercall2 which descheduled the domain and caused Xen tosend a notification to xenstore, which then sent an interrupt to xend, which finally returned control to the migra-tion process. This convoluted process could take a nearly
arbitrary amount of time -- typical measured latency wasin the range of 30 to 40ms, but we saw delays as long as
500ms in some cases.

Remus's optimized suspend code streamlines this pro-cess by creating an event channel in the guest specifically

for receiving suspend requests, which the migration pro-cess can invoke directly. Additionally, a new hypercall is
provided to allow processes to register an event channelfor callbacks notifying them of the completion of VM
suspension. In concert, these two notification mecha-nisms reduce the time required to suspend a VM to about
one hundred microseconds - an improvement of two or-ders of magnitude over the previous implementation.

In addition to these signaling changes, we have in-creased the efficiency of the memory copying process.
First, we quickly filter out clean pages from the mem-ory scan, because at high checkpoint frequencies most
memory is unchanged between rounds. Second, wemap the guest domain's entire physical memory into the
replication process when it begins, rather than mappingand unmapping dirty pages at every epoch -- we found
that mapping foreign pages took approximately the sametime as copying them.

Checkpoint support. Providing checkpoint supportin Xen required two primary changes to the existing
suspend-to-disk and live migration code. First, supportwas added for resuming execution of a domain after it
had been suspended; Xen previously did not allow "livecheckpoints" and instead destroyed the VM after writing its state out. Second, the suspend program was con-verted from a one-shot procedure into a daemon process.
This allows checkpoint rounds after the first to copy onlynewly-dirty memory.

Supporting resumption requires two basic changes.The first is a new hypercall to mark the domain as
schedulable again (Xen removes suspended domainsfrom scheduling consideration, because previously they
were always destroyed after their state had been repli-cated). A similar operation is necessary in order to rearm watches in xenstore.Asynchronous transmission.

To allow the guest toresume operation as quickly as possible, the migration

process was modified to copy touched pages to a stagingbuffer rather than delivering them directly to the network
while the domain is paused. This results in a signifi-cant throughput increase: the time required for the kernel
build benchmark discussed in Section 3.3 was reduced byapproximately 10% at 20 checkpoints per second.

Guest modifications. As discussed above, paravirtualguests in Xen contain a suspend handler that cleans up
device state upon receipt of a suspend request. In addi-tion to the notification optimizations described earlier in
this section, the suspend request handler has also beenmodified to reduce the amount of work done prior to suspension. In the original code, suspension entailed dis-connecting all devices and unplugging all but one CPU.
This work was deferred until the domain was restored onthe other host. These modifications are available in Xen
as of version 3.1.0.

These changes are not strictly required for correctness,but they do improve the performance of the checkpoint

considerably, and involve very local modifications to theguest kernel. Total changes were under 100 lines of code
in the paravirtual suspend handler. As mentioned earlier,these modifications are not necessary in the case of nonparavirtualized VMs.

2.4 Network buffering
Most networks cannot be counted on for reliable datadelivery. Therefore, networked applications must either accept packet loss, duplication and reordering, oruse a high-level protocol such as TCP which provides
stronger service guarantees. This fact simplifies the net-work buffering problem considerably: transmitted packets do not require replication, since their loss will appearas a transient network failure and will not affect the correctness of the protected state. However, it is crucial thatpackets queued for transmission be held until the checkpointed state of the epoch in which they were generatedis committed to the backup; if the primary fails, these
generated packets reflect speculative state that has beenlost.

Figure 3 depicts the mechanism by which we preventthe release of speculative network state. Inbound traffic is delivered to the protected host immediately, butoutbound packets generated since the previous checkpoint are queued until the current state has been check-pointed and that checkpoint has been acknowledged by
the backup site. We have implemented this buffer as alinux queuing discipline applied to the guest domain's
network device in domain 0, which responds to two RT-netlink messages. Before the guest is allowed to resume
execution after a checkpoint, the network buffer receivesa

CHECKPOINT message, which causes it to insert a bar-rier into the outbound queue preventing any subsequent

packets from being released until a corresponding re-lease message is received. When a guest checkpoint has
been acknowledged by the backup, the buffer receives a
RELEASE message, at which point it begins dequeueingtraffic up to the barrier.

There are two minor wrinkles in this implementation.The first is that in linux, queueing disciplines only operate on outgoing traffic. Under Xen, guest network in-terfaces consist of a frontend device in the guest, and a
corresponding backend device in domain 0. Outboundtraffic from the guest appears as

inbound traffic on thebackend device in domain 0. Therefore in order to queue

the traffic, we convert the inbound traffic to outbound byrouting it through a special device called an

intermediatequeueing device [16]. This module is designed to work

at the IP layer via iptables [27], but it was not difficult toextend it to work at the bridging layer we use to provide
VM network access in our implementation.

The second wrinkle is due to the implementation of theXen virtual network device. For performance, the memory used by outbound networking traffic is not copiedbetween guest domains and domain 0, but shared. However, only a small number of pages may be shared at anyone time. If messages are in transit between a guest and
domain 0 for only a brief time, this limitation is not no-ticeable. Unfortunately, the network output buffer can
result in messages being in flight for a significant amountof time, which results in the guest network device blocking after a very small amount of traffic has been sent.Therefore when queueing messages, we first copy them
into local memory and then release the local mappings toshared data.

2.5 Disk buffering
Disks present a rather different challenge than networkinterfaces, largely because they are expected to provide

much stronger reliability guarantees. In particular, whena write has been acknowledged by a disk, an application (or file system) expects to be able to recover thatdata even in the event of a power failure immediately folBu\Gamma \Gamma e\Delta 

Cl\Theta e\Lambda t
P\Delta \Theta i\Xi \Delta y \Pi o\Sigma t

V\Upsilon 

Figure 3: Network buffering in Remus.
lowing the acknowledgment. While Remus is designedto recover from a single host failure, it must preserve
crash consistency even if both hosts fail. Moreover, thegoal of providing a general-purpose system precludes the
use of expensive mirrored storage hardware designed forHA applications. Therefore Remus maintains a complete
mirror of the active VM's disks on the backup host. Priorto engaging the protection system, the current state of
the disk on the primary is mirrored to the backup host.Once protection has been engaged, writes to persistent
storage are tracked and checkpointed similarly to updatesto memory. Figure 4 gives a high-level overview of the
disk replication mechanism

As with the memory replication subsystem describedin Section 2.3, writes to disk from the active VM are

treated as write-through: they are immediately applied tothe primary disk image, and asynchronously mirrored to
an in-memory buffer on the backup. This approach pro-vides two direct benefits: First, it ensures that the active
disk image remains crash consistent at all times; in thecase of both hosts failing, the active disk will reflect the
crashed state of the externally visible VM at the time offailure (the externally visible VM resides on the primary
host if the primary host has not failed or if the backupalso fails before it has been activated, otherwise it resides on the backup). Second, writing directly to diskaccurately accounts for the latency and throughput characteristics of the physical device. This obvious-seemingproperty is of considerable value: accurately characterizing disk responsiveness is a subtle problem, as we our-selves experienced in an earlier version of the disk buffer
which held write requests in memory on the primaryVM until checkpoint commit. Such an approach either
buffers writes, under-representing the time required tocommit data to disk and allowing the speculating VM to
race ahead in execution, or conservatively over-estimateswrite latencies resulting in a loss of performance. Modeling disk access time is notoriously challenging [28], but

Pri\Gamma \Delta ry
Ho\Theta \Lambda 

1

2

Bu
\Xi \Xi e
\Pi 

S\Sigma \Upsilon o\Phi \Psi \Delta ry

Ho\Theta \Lambda 

3
1 D\Omega fffi flffi\Omega ffliff jffii \Omega ffff`i''\Omega ffii^fflt* ffl_ t_^jt '\Omega fffi
2 *\Omega ,`tffljssi_`fft* ffissfflffl_ aej^fi`oe ae`o/iffi
3 Wffi\Omega ffliff ffiitijffi' ffl_ '\Omega fffijaffliffi ^AEi^fioe_\Omega ssffl

Figure 4: Disk write buffering in Remus.
our implementation avoids the problem by preserving di-rect feedback from the disk to its client VM.

At the time that the backup acknowledges that a check-point has been received, disk updates reside completely
in memory. No on-disk state may be changed until theentire checkpoint has been received, as this would prevent the backup from rolling back to the most recentcomplete checkpoint. Once the checkpoint is acknowledged, the disk request buffer may be applied to disk. Inthe event of a failure, Remus will wait until all buffered
writes have been applied before resuming execution. Al-though the backup could begin execution immediately
using the request buffer as an overlay on the physicaldisk, this would violate the disk semantics presented to
the protected VM: if the backup fails after activation butbefore data is completely flushed to disk, its on-disk state
might not be crash consistent.

Only one of the two disk mirrors managed by Remusis actually valid at any given time. This point is critical in recovering from multi-host crashes. This prop-erty is achieved by the use of an activation record on the
backup disk, which is written after the most recent diskbuffer has been completely flushed to disk and before the
backup VM begins execution. In recovering from multi-ple host failures, this record may be used to identify the
valid, crash consistent version of the disk.

The disk buffer is implemented as a Xen block tapmodule [32]. The block tap is a device which allows

a process in the privileged domain to efficiently inter-pose itself between the frontend disk device presented
to a guest VM and the backend device which actuallyservices requests. The buffer module logs disk write requests from the protected VM and mirrors them to a cor-responding module on the backup, which executes the
checkpoint protocol described above and then removesitself from the disk request path before the backup begins execution in the case of failure at the primary.

2.6 Detecting Failure
Remus's focus is on demonstrating that it is possible toprovide advanced high availability in a general and transparent way using commodity hardware and without mod-ifying the protected applications. We currently use a
simple failure detector that is directly integrated in thecheckpointing stream: a timeout of the backup responding to commit requests will result in the primary assum-ing that the backup has crashed and disabling protection.
Similarly, a timeout of new checkpoints being transmit-ted from the primary will result in the backup assuming that the primary has crashed and resuming executionfrom the most recent checkpoint.

The system is configured to use a pair of bonded net-work interfaces, and the two physical hosts are connected
using a pair of Ethernet crossover cables (or independentswitches) on the protection NICs. Should both of these
network paths fail, Remus does not currently providemechanism to fence execution. Traditional techniques
for resolving partitioning (i.e., quorum protocols) are no-toriously difficult to apply in two host configurations. We
feel that in this case, we have designed Remus to the edgeof what is possible with commodity hardware.

3 Evaluation
Remus has been designed with the primary objective ofmaking high availability sufficiently generic and transparent that it may be deployed on today's commodityhardware. In this section, we characterize the overheads
resulting from our approach for a variety of differentworkloads, in order two answer two questions: (1) Is this
system practically deployable? (2) What kinds of work-loads are most amenable to our approach?

Before measuring the performance impact, we mustestablish that the system functions correctly. We accomplish this by injecting network failures at each phase ofthe replication protocol, while putting substantial disk,
network and CPU load on the protected system. We findthat the backup takes over for the lost primary within approximately one second in every case, preserving all ex-ternally visible state, including active network connections.

We then evaluate the overhead of the system on appli-cation performance across very different workloads. We

find that a general-purpose task such as kernel compi-lation incurs approximately a 50% performance penalty
when checkpointed 20 times per second, while network-dependent workloads as represented by SPECweb perform at somewhat more than one quarter native speed.The additional overhead in this case is largely due to
output-commit delay on the network interface.

Based on this analysis, we conclude that although Remus is efficient at state replication, it does introduce sig-nificant network delay, particularly for applications that
exhibit poor locality in memory writes. Thus, appli-cations that are very sensitive to network latency may
not be well suited to this type of high availability ser-vice (although there are a number of optimizations which
have the potential to noticeably reduce network delay,some of which we discuss in more detail following the
benchmark results). We feel that we have been conser-vative in our evaluation, using benchmark-driven workloads which are significantly more intensive than wouldbe expected in a typical virtualized system; the consolidation opportunities such an environment presents areparticularly attractive because system load is variable.

3.1 Test environment
Unless otherwise stated, all tests were run on IBM eS-erver x306 servers, consisting of one 3.2 GHz Pentium 4

processor with hyperthreading enabled, 1 GB of RAM, 3Intel e1000 GbE network interfaces, and an 80 GB SATA
hard drive. The hypervisor was Xen 3.1.2, modified asdescribed in Section 2.3, and the operating system for all
virtual machines was linux 2.6.18 as distributed in Xen3.1.2, with the modifications described in Section 2.3.
The protected VM was allocated 512 MB of total RAM.To minimize scheduling effects from the VMM, domain
0's VCPU was pinned to the first hyperthread. One phys-ical network interface was bridged to the guest virtual interface and used for application traffic, one was used foradministrative access, and the last was used for replication (we did not bond interfaces for replication, but this isimmaterial to the tests we performed). Virtual disks were
provided by disk images on the SATA drive, exported tothe guest using the tapdisk AIO driver.

3.2 Correctness verification
As discussed in Section 2.2, Remus's replication pro-tocol operates in four distinct phases: (1) checkpoint

changed state and increment the epoch of network anddisk request streams, (2) replicate system state, (3) when
the complete memory checkpoint and corresponding setof disk requests has been received, send a checkpoint acknowledgement from the backup, and (4) upon receipt ofthe acknowledgement, release outbound network packets
queued during the previous epoch. To verify that our sys-tem functions as intended, we tested deliberately induced
network failure at each stage. For each test, the protectedsystem executed a kernel compilation process in order
to generate disk, memory and CPU load. To verify thenetwork buffer, we simultaneously executed a graphicsintensive X11 client (glxgears) attached to an externalX11 server. Remus was configured to take checkpoints

Pages dirtied
1 256 512 1024

Mil
lise

con
ds

0
10
20
30
40
50
60
70
80 time suspended

time transmitting

Figure 5: Checkpoint time relative to pages dirtied.
every 25 milliseconds throughout. Each individual testwas repeated twice.

At every failure point, the backup successfully tookover execution of the protected system, with only minor network delay (about one second) noticeable whilethe backup detected the failure and activated the replicated system. The glxgears client continued to run aftera brief pause, and the kernel compilation task continued
to successful completion. We then gracefully shut downthe VM and executed a forced file system check on the
backup disk image, which reported no inconsistencies.

3.3 Benchmarks
In the following section, we evaluate the performance ofour system using a variety of macrobenchmarks which

are meant to be representative of a range of real-worldworkload mixtures. The primary workloads we run are a
kernel compilation test, the SPECweb2005 benchmark,and the Postmark disk benchmark. Kernel compilation is
a balanced workload which stresses the virtual memorysystem, the disk and the CPU, SPECweb primarily exercises networking performance and memory throughput,and Postmark focuses on disk performance.

To better understand the following measurements, weperformed a microbenchmark measuring the time spent
copying guest state (while the guest was suspended) andthe time spent sending that data to the backup relative to
the number of pages changed since the previous check-point. We wrote an application to repeatedly change the
first byte of a set number of pages and measured timesover 1000 iterations. Figure 5 presents the average, minimum and maximum recorded times spent in the checkCheckpoints per second
0 10 20 30 40

Ker
nel
 bu
ild 
tim
e (s

eco
nds
)

025
5075
100125
150175
200225
250275
300325
350375
400425
450475
500525
550575
600625
650

Figure 6: Kernel build time by checkpoint frequency.
point and replication stages, within a 95% confidence in-terval. It shows that the bottleneck for checkpoint frequency is replication time.Kernel compilation.

The kernel compile test mea-sures the wall-clock time required to build linux kernel

version 2.6.18 using the default configuration and the bzImage target. Compilation uses GCC version 4.1.2, andmake version 3.81. This is a balanced workload that tests

CPU, memory and disk performance.

Figure 6 shows protection overhead when configuredto checkpoint at rates of 10, 20, 30 and 40 times per second, compared to a baseline compilation in an unpro-tected virtual machine. Total measured overhead at each
of these frequencies was 31%, 52%, 80% and 103%, re-spectively. Overhead scales linearly with checkpoint frequency within the rates we tested. We believe that theoverhead measured in this set of tests is reasonable for a
general-purpose system, even at a checkpoint frequencyof 40 times per second.

SPECweb2005. The SPECweb benchmark is com-posed of at least three separate systems: a web server,
an application server, and one or more web client sim-ulators. We configure these as three VMs on distinct
physical machines. The application server and the clientare configured with 640 MB out of 1024 MB total available RAM. The web server and backup are provisionedwith 2048 MB of RAM, of which 1024 is allocated
to the web server VM, which is the system under test.The SPECweb scores we mention in this section are
the highest results we achieved with the SPECweb "e-commerce" test maintaining 95% "good" and 99% "tolerable" times.

Checkpoints per second
0 10 20 30 40

SP
EC
we
b20

05 
sco

re

0
10
20
30
40
50
60
70
80
90
100
110
120
130
140
150
160
170
180
190 with netbuf no netbuf

Figure 7: SPECweb scores by checkpoint frequency (na-tive score: 305)

Figure 7 shows SPECweb performance at variouscheckpoint frequencies relative to an unprotected server.
These scores are primarily a function of the delay im-posed by the network buffer between the server and
the client. Although they are configured for a range offrequencies, SPECweb touches memory rapidly enough
that the time required to propagate the memory dirtiedbetween checkpoints sometimes exceeds 100ms, regardless of checkpoint frequency. Because the network buffercannot be released until checkpointed state has been acknowledged, the effective network delay can be higherthan the configured checkpoint interval. Remus does ensure that the VM is suspended at the start of every epoch,but it cannot currently ensure that the total amount of
state to be replicated per epoch does not exceed the band-width available during the configured epoch length. Because the effective checkpoint frequency is lower thanthe configured rate, and network latency dominates the
SPECweb score, performance is relatively flat across therange of configured frequencies. At configured rates of
10, 20, 30 and 40 checkpoints per second, the averagecheckpoint rates achieved were 9.98, 16.38, 20.25 and
23.34 respectively, or average latencies of 100ms, 61ms,49ms and 43ms respectively.

SPECweb is a RAM-hungry workload which is alsovery sensitive to network latency. This makes it a poor
fit for our current implementation, which trades networkdelay for memory throughput. Figure 8 demonstrates the
dramatic effect delay between the client VM and the webserver has on SPECweb. We used the Linux netem [19]
queueing discipline to add varying degrees of delay to

Network latency (ms)
0 60 70 80 90 100

SP
EC
we
b20

05 
sco

re

0
15
30
45
60
75
90
105
120
135
150
165
180
195
210
225
240
255
270
285
300
315

Figure 8: The effect of network delay on SPECweb per-formance.
the outbound link from the web server (virtualized butnot running under Remus). For comparison, Figure 7
also shows protection overhead when network buffer-ing is disabled, to better isolate network latency from
other forms of checkpoint overhead (again, the flat pro-file is due to the effective checkpoint rate falling short
of the configured rate). Deadline scheduling and pagecompression, discussed in Section 3.4 are two possible
techniques for reducing checkpoint latency and transmis-sion time. Either or both would reduce checkpoint latency, and therefore be likely to increase SPECweb per-formance considerably.

Postmark. The previous sections characterize net-work and memory performance under protection, but the
benchmarks used put only moderate load on the disksubsystem. In order to better understand the effects of
the disk buffering mechanism, we ran the Postmark diskbenchmark (version 1.51). This benchmark is sensitive
to both throughput and disk response time. To isolate thecost of disk replication, we did not engage memory or
network protection during these tests. Configuration wasidentical to an unprotected system, with the exception
that the virtual disk was provided by the tapdisk replica-tion module. Figure 9 shows the total time required to
perform 10000 postmark transactions with no disk repli-cation, and with a replicated disk committing at frequencies of 10, 20, 30 and 40 times per second. The resultsindicate that replication has no significant impact on disk
performance.

Checkpoints per second
0 10 20 30 40

Tim
e p
er 1

000
0 tr
ans

act
ion
s (s

eco
nds
)

0
25
50
75
100
125
150
175
200
225
250
275
300
325
350
375

Figure 9: The effect of disk replication of Postmark per-formance.
3.4 Potential optimizations
Although we believe the performance overheads shownearlier in this section are reasonable for what they provide, we are eager to reduce them further, particularly forlatency-sensitive workloads. In addition to more careful tuning of the existing code, we believe the followingtechniques have the potential to greatly increase performance.Deadline scheduling.

The amount of time requiredto perform a checkpoint is currently variable, depending on the amount of memory to be copied. AlthoughRemus ensures that the protected VM is suspended at
the start of each epoch, it currently makes no attemptto control the amount of state which may change between epochs. To provide stricter scheduling guaran-tees, the rate at which the guest operates could be deliberately slowed [10] between checkpoints, depending onthe number of pages dirtied. Applications which prioritize latency over throughput, such as those modeled bythe SPECweb benchmark discussed in Section 3.3, may
enable this throttling for improved performance. To per-form such an operation, the shadow page table handler
could be extended to invoke a callback when the numberof dirty pages exceeds some high water mark, or it may
be configured to pause the virtual machine directly.Page compression.

It has been observed that diskwrites typically only alter 5-20% of a data block [35].

If a similar property holds for RAM, we may exploit it inorder to reduce the amount of state requiring replication,
by sending only the delta from a previous transmissionof the same page.

Time
Ban
dw
idth

 (M
B/s
)

0
5
10
15
20
25
30
35
40
45
50
55
60
65 Raw

XOR

GZIP
Hybrid

Figure 10: Comparison of bandwidth requirements usingvarious compression schemes.

To evaluate the potential benefits of compressing thereplication stream, we have prototyped a basic compression engine. Before transmitting a page, this systemchecks for its presence in an address-indexed LRU cache
of previously transmitted pages. On a cache hit, the pageis XORed with the previous version and the differences
are run-length encoded. This provides significant com-pression when page writes do not change the majority
of the page. Although this is true for much of the datastream, there remains a significant fraction of pages that
have been modified to the point where XOR compres-sion is not effective. In these cases, a general-purpose
algorithm such as that used by gzip may achieve a higherdegree of compression.

We found that by using a hybrid approach, in whicheach page is preferentially XOR-compressed, but falls
back to gzip compression if the XOR compression ra-tio falls below 5:1 or the previous page is not present in
the cache, we could observe a typical compression ratioof 10:1 on the replication stream. Figure 10 shows the
bandwidth consumed in MBps for a 60-second periodof the kernel compilation benchmark described in Section 3.3. The cache size was 8192 pages and the averagecache hit rate was 99%.

Compressing the replication stream can consume ad-ditional memory and CPU resources on the replicating
host, but lightweight schemes such as the XOR compres-sion technique should pay for themselves through the reduction in bandwidth required for replication and conse-quent reduction in network buffering delay.

Copy-on-write checkpoints. The current implementation pauses the domain at each checkpoint for anamount of time linear in the number of pages which have
been dirtied since the last checkpoint. This overheadcould be mitigated by marking dirty pages as copy-onwrite and resuming the domain immediately. This wouldreduce the time during which the domain must be paused
to a fixed small cost proportional to the total amountof RAM available to the guest. We intend to implement copy-on-write by supplying the Xen shadow pag-ing system with a userspace-mapped buffer into which
it could copy touched pages before restoring read-writeaccess. The replication process could then extract any
pages marked as copied from the COW buffer instead ofreading them directly from the guest. When it had finished replicating pages, their space in the buffer could bemarked for reuse by the Xen COW module. If the buffer
were to become full, the guest could simply be paused,resulting in a graceful degradation of service from COW
to stop-and-copy operation.

4 Related Work
State replication may be performed at several levels, eachof which balances efficiency and generality differently.
At the lowest level, hardware-based replication is poten-tially the most robust solution. Hardware, however, is
much more expensive to develop than software and thushardware replication is at a significant economic disadvantage. Replication at the virtualization layer has manyof the advantages of the hardware approach, but comes
at lower cost because it is implemented in software. Likehardware, however, the virtualization layer has no semantic understanding of the operating-system and appli-cation state it replicates. As a result it can be less flexible than process checkpointing in the operating system,in application libraries or in applications themselves, because it must replicate the entire system instead of indi-vidual processes. It can also be less efficient, because it
may replicate unnecessary state. The challenge for thesehigher-level approaches, however, is that interdependencies among state elements that comprise a checkpoint areinsidiously difficult to identify and untangle from the rest
of the system and thus these checkpointing mechanismsare significantly more complex than checkpointing in the
virtualization layer.Virtual machine migration. As described earlier, Remus is built on top of the Xen support for live migra-tion [6], extended significantly to support frequent, remote checkpointing. Bradford et al. extended Xen's livemigration support in another direction: migrating persistent state along with the migrating guest so that it can berestarted on a remote node that does not share network
storage with the originating system[3].Like Remus, other projects have used virtual machines

to provide high availability. The closest to our work isBressoud and Schneider's [4]. They use the virtual machine monitor to forward the input events seen by a pri-mary system to a backup system where they are deterministically replayed to replicate the primary's state. De-terministic replay requires much stricter constraints on
the target architecture than simple virtualization and itrequires an architecture- specific implementation in the
VMM.

Another significant drawback of deterministic replayas exemplified by Bressoud and Schneider's work is that

it does not easily extend to multi-core CPUs. The prob-lem is that it is necessary, but difficult, to determine the
order in which cores access shared memory. There havebeen some attempts to address this problem. For example, Flight Data Recorder [34] is a hardware modulethat sniffs cache coherency traffic in order to record the
order in which multiple processors access shared mem-ory. Similarly, Dunlap introduces a software approach in
which the CREW protocol (concurrent read, exclusivewrite) is imposed on shared memory via page protection [8]. While these approaches do make SMP deter-ministic replay possible, it is not clear if they make it
feasible due to their high overhead, which increases atleast linearly with the degree of concurrency. Our work
sidesteps this problem entirely because it does not re-quire deterministic replay.

Virtual machine logging and replay. Virtual ma-chine logging has been used for purposes other than high
availability. For example, in ReVirt [9], virtualization isused to provide a secure layer for logging state changes
in the target system in order to provide better forensic ev-idence for intrusion detection systems. The replayed system is a read-only copy of the original system, which isnot meant to be run except in order to recreate the events
involved in a system compromise. Logging has also beenused to build a

time-travelling debugger [13] that, likeReVirt, replays the system for forensics only.

Operating system replication. There are many op-erating systems, such as

Accent [25], Amoeba [18],
MOSIX [1] and Sprite [23], which support process mi-gration, mainly for load balancing. The main challenge

with using process migration for failure recovery is thatmigrated processes typically leave residual dependencies
to the system from which they were migrated. Eliminat-ing these dependencies is necessary to tolerate the failure
of the primary host, but the solution is elusive due to thecomplexity of the system and the structure of these dependencies.

Some attempts have been made to replicate applica-tions at the operating system level.

Zap [22] attemptsto introduce a virtualization layer within the linux kernel. This approach must be rebuilt for every operatingsystem, and carefully maintained across versions.

Library approaches. Some application libraries pro-vide support for process migration and checkpointing.
This support is commonly for parallel application frame-works such as CoCheck [29]. Typically process migration is used for load balancing and checkpointing is usedto recover an entire distributed application in the event of
failure.Replicated storage. There has also been a large
amount of work on checkpointable storage for disasterrecovery as well as forensics. The Linux Logical Volume Manager [14] provides a limited form of copy-on-write snapshots of a block store. Parallax [33] significantly improves on this design by providing limitlesslightweight copy-on-write snapshots at the block level.
The Andrew File System [11] allows one snapshot at atime to exist for a given volume. Other approaches include RSnapshot, which runs on top of a file system tocreate snapshots via a series of hardlinks, and a wide variety of backup software. DRBD [26] is a software ab-straction over a block device which transparently replicates it to another server.Speculative execution. Using speculative execution
to isolate I/O processing from computation has been ex-plored by other systems. In particular, SpecNFS [20] and
Rethink the Sync [21] use speculation in a manner sim-ilar to us in order to make I/O processing asynchronous.
Remus is different from these systems in that the se-mantics of block I/O from the guest remain entirely unchanged: they are applied immediately to the local phys-ical disk. Instead, our system buffers generated network
traffic to isolate the externally visible effects of specu-lative execution until the associated state has been completely replicated.

5 Future work
This section briefly discusses a number of directions thatwe intend to explore in order to improve and extend Remus. As we have demonstrated in the previous section,the overhead imposed by our high availability service
is not unreasonable. However, the implementation de-scribed in this paper is quite young. Several potential
areas of optimization remain to be explored. Upon com-pletion of the targeted optimizations discussed in Section 3.4, we intend to investigate more general extensionssuch as those described below.

Introspection optimizations. Remus currently prop-agates more state than is strictly necessary. For example,
buffer cache pages do not need to be replicated, sincethey can simply be read in from persistent storage on the
backup. To leverage this, the virtual disk device couldlog the addresses of buffers provided to it for disk reads,
along with the associated disk addresses. The memory-copying process could then skip over these pages if they

had not been modified after the completion of the diskread. The remote end would be responsible for reissuing
the reads from its copy of the disk in order to fill in themissing pages. For disk-heavy workloads, this should result in a substantial reduction in state propagation time.Hardware virtualization support. Due to the lack
of equipment supporting hardware virtualization in ourlaboratory at the time of development, we have only implemented support for paravirtualized guest virtual ma-chines. But we have examined the code required to support fully virtualized environments, and the outlook isquite promising. In fact, it may be somewhat simpler
than the paravirtual implementation due to the better en-capsulation provided by virtualization-aware hardware.

Cluster replication. It would be useful to extend thesystem to protect multiple interconnected hosts. While
each host can be protected independently, coordinatedprotection would make it possible for internal network
communication to proceed without buffering. This hasthe potential to dramatically improve the throughput of
distributed applications, including the three-tiered webapplication configuration prevalent in managed hosting
environments. Support for cluster replication could beprovided by a distributed checkpointing protocol such
as that which is described in our colleague Gang Peng'smaster's thesis [24], which used an early version of the
checkpointing infrastructure provided by Remus.Disaster recovery. Remus is a product of the SecondSite [7] project, whose aim was to provide geographi-cally diverse mirrors of running systems in order survive
physical disaster. We are in the process of planning amulti-site deployment of Remus in order to experiment
with this sort of configuration. In a long distance de-ployment, network delay will be an even larger concern.
Additionally, network reconfigurations will be requiredto redirect Internet traffic accordingly.

Log-structured datacenters. We are extending Re-mus to capture and preserve the complete execution history of protected VMs, rather than just the most re-cent checkpoint. By mapping guest memory into Parallax [17], our virtual block store designed for high-frequency snapshots, we hope to be able to efficiently
store large amounts of both persistent and transient stateat very fine granularity. This data should be very useful in building advanced debugging and forensics tools.It may also provide a convenient mechanism for recovering from state corruption whether introduced by operatorerror or by malicious agents (viruses and so forth).

6 Conclusion
Remus is a novel system for retrofitting high availabilityonto existing software running on commodity hardware.
The system uses virtualization to encapsulate a protected

VM, and performs frequent whole-system checkpointsto asynchronously replicate the state of a single speculatively executing virtual machine.Providing high availability is a challenging task and
one that has traditionally required considerable cost andengineering effort. Remus commodifies high availability by presenting it as a service at the virtualization plat-form layer: HA may simply be "switched on" for specific virtual machines. As with any HA system, protec-tion does not come without a cost: The network buffering required to ensure consistent replication imposes aperformance overhead on applications that require very
low latency. Administrators must also deploy additionalhardware, which may be used in N-to-1 configurations
with a single backup protecting a number of active hosts.In exchange for this overhead, Remus completely eliminates the task of modifying individual applications inorder to provide HA facilities, and it does so without requiring special-purpose hardware.Remus represents a previously unexplored point in the
design space of HA for modern servers. The system al-lows protection to be simply and dynamically provided
to running VMs at the push of a button. We feel thatthis model is particularly attractive for hosting providers,
who desire to offer differentiated services to customers.

Acknowledgments
The authors would like to thank their paper shepherd,Arun Venkataramani, and the anonymous reviewers for
their insightful and encouraging feedback. We are alsoindebted to Anoop Karollil for his aid in the evaluation
process. This work is supported by grants from IntelResearch and the National Science and Engineering Research Council of Canada.

References

[1] BARAK, A., AND WHEELER, R. Mosix: an integrated multipro-cessor unix. 41-53.

[2] BARHAM, P., DRAGOVIC, B., FRASER, K., HAND, S.,H

ARRIS, T., HO, A., NEUGEBAUER, R., PRATT, I., ANDW

ARFIELD, A. Xen and the art of virtualization. In SOSP'03: Proceedings of the nineteenth ACM symposium on Operating systems principles (New York, NY, USA, 2003), ACM Press,pp. 164-177.

[3] BRADFORD, R., KOTSOVINOS, E., FELDMANN, A., ANDS

CHI "OBERG, H. Live wide-area migration of virtual machinesincluding local persistent state. In VEE '07: Proceedings of the

3rd international conference on Virtual execution environments(New York, NY, USA, 2007), ACM Press, pp. 169-179.

[4] BRESSOUD, T. C., AND SCHNEIDER, F. B. Hypervisor-basedfault-tolerance. In Proceedings of the Fifteenth ACM Symposium

on Operating System Principles (December 1995), pp. 1-11.
[5] CHANDRA, S., AND CHEN, P. M. The impact of recoverymechanisms on the likelihood of saving corrupted state. In ISSRE '02: Proceedings of the 13th International Symposium on

Software Reliability Engineering (ISSRE'02) (Washington, DC,USA, 2002), IEEE Computer Society, p. 91.
[6] CLARK, C., FRASER, K., HAND, S., HANSEN, J. G., JUL, E.,L

IMPACH, C., PRATT, I., AND WARFIELD, A. Live migration ofvirtual machines. In Proceedings of the 2nd conference on Symposium on Networked Systems Design & Implementation (Berke-ley, CA, USA, 2005), USENIX Association.

[7] CULLY, B., AND WARFIELD, A. Secondsite: disaster protectionfor the common server. In HOTDEP'06: Proceedings of the 2nd

conference on Hot Topics in System Dependability (Berkeley, CA,USA, 2006), USENIX Association.

[8] DUNLAP, G. Execution Replay for Intrusion Analysis. PhD the-sis, University of Michigan, 2006.
[9] DUNLAP, G. W., KING, S. T., CINAR, S., BASRAI, M. A., ANDC

HEN, P. M. Revirt: Enabling intrusion analysis through virtual-machine logging and replay. In Proceedings of the 5th Symposium on Operating Systems Design & Implementation (OSDI2002) (2002).

[10] GUPTA, D., YOCUM, K., MCNETT, M., SNOEREN, A. C.,V

AHDAT, A., AND VOELKER, G. M. To infinity and beyond:time warped network emulation. In SOSP '05: Proceedings of

the twentieth ACM symposium on Operating systems principles(2005).

[11] HOWARD, J. H., KAZAR, M. L., MENEES, S. G., NICHOLS,D. A., S

ATYANARAYANAN, M., SIDEBOTHAM, R. N., ANDW
EST, M. J. Scale and performance in a distributed file system.ACM Transactions on Computer Systems 6, 1 (1988), 51-81.

[12] HP. NonStop Computing. http://h20223.www2.hp.com/non-stopcomputing/cache/76385-0-0-0-121.aspx.
[13] KING, S. T., DUNLAP, G. W., AND CHEN, P. M. Debug-ging operating systems with time-traveling virtual machines. In

ATEC'05: Proceedings of the USENIX Annual Technical Confer-ence 2005 on USENIX Annual Technical Conference (Berkeley,
CA, USA, 2005), USENIX Association.
[14] Lvm2. http://sources.redhat.com/lvm2/.
[15] MARQUES, D., BRONEVETSKY, G., FERNANDES, R., PINGALI, K., AND STODGHILL, P. Optimizing checkpoint sizesin the c3 system. In 19th International Parallel and Distributed

Processing Symposium (IPDPS 2005) (April 2005).
[16] MCHARDY, P. Linux imq. http://www.linuximq.net/.
[17] MEYER, D., AGGARWAL, G., CULLY, B., LEFEBVRE, G.,H

UTCHINSON, N., FEELEY, M., AND WARFIELD, A. Parallax:Virtual disks for virtual machines. In EuroSys '08: Proceedings

of the ACM SIGOPS/EuroSys European Conference on ComputerSystems 2008 (New York, NY, USA, 2008), ACM.

[18] MULLENDER, S. J., VAN ROSSUM, G., TANENBAUM, A. S.,

VAN RENESSE, R., AND VAN STAVEREN, H. Amoeba: A dis-tributed operating system for the 1990s. Computer 23, 5 (1990),

44-53.
[19] netem. http://linux-net.osdl.org/index.php/Netem.
[20] NIGHTINGALE, E. B., CHEN, P. M., AND FLINN, J. Specu-lative execution in a distributed file system. In SOSP '05: Proceedings of the twentieth ACM symposium on Operating systemsprinciples (New York, NY, USA, 2005), ACM Press, pp. 191-
205.
[21] NIGHTINGALE, E. B., VEERARAGHAVAN, K., CHEN, P. M.,

AND FLINN, J. Rethink the sync. In USENIX'06: Proceed-ings of the 7th conference on USENIX Symposium on Operating

Systems Design and Implementation (Berkeley, CA, USA, 2006),USENIX Association.

[22] OSMAN, S., SUBHRAVETI, D., SU, G., AND NIEH, J. Thedesign and implementation of zap: a system for migrating computing environments. SIGOPS Oper. Syst. Rev. 36, SI (2002),361-376.

[23] OUSTERHOUT, J. K., CHERENSON, A. R., DOUGLIS, F., NELSON, M. N., AND WELCH, B. B. The sprite network operatingsystem. Computer 21, 2 (1988), 23-36.

[24] PENG, G. Distributed checkpointing. Master's thesis, Universityof British Columbia, 2007.
[25] RASHID, R. F., AND ROBERTSON, G. G. Accent: A communi-cation oriented network operating system kernel. In SOSP '81:

Proceedings of the eighth ACM symposium on Operating systemsprinciples (New York, NY, USA, 1981), ACM Press, pp. 64-75.

[26] REISNER, P., AND ELLENBERG, L. Drbd v8 - replicated storagewith shared disk semantics. In Proceedings of the 12th International Linux System Technology Conference (October 2005).
[27] RUSSELL, R. Netfilter. http://www.netfilter.org/.
[28] SCHINDLER, J., AND GANGER, G. Automated disk drive char-acterization. Tech. Rep. CMU SCS Technical Report CMU-CS99-176, Carnegie Mellon University, December 1999.
[29] STELLNER, G. CoCheck: Checkpointing and Process Migrationfor MPI. In Proceedings of the 10th International Parallel Processing Symposium (IPPS '96) (Honolulu, Hawaii, 1996).
[30] SYMANTEC CORPORATION. Veritas Cluster Server for VMwareESX. http://eval.symantec.com/mktginfo/products/Datasheets

/High Availability/vcs22vmware datasheet.pdf, 2006.
[31] VMWARE, INC. Vmware high availability (ha).http://www.vmware.com/products/vi/vc/ha.html, 2007.

[32] WARFIELD, A. Virtual Devices for Virtual Machines. PhD thesis,University of Cambridge, 2006.
[33] WARFIELD, A., ROSS, R., FRASER, K., LIMPACH, C., ANDH

AND, S. Parallax: managing storage for a million machines.In HOTOS'05: Proceedings of the 10th conference on Hot Topics in Operating Systems (Berkeley, CA, USA, 2005), USENIXAssociation.

[34] XU, M., BODIK, R., AND HILL, M. D. A "flight data recorder"for enabling full-system multiprocessor deterministic replay. In

ISCA '03: Proceedings of the 30th annual international sym-posium on Computer architecture (New York, NY, USA, 2003),
ACM Press, pp. 122-135.
[35] YANG, Q., XIAO, W., AND REN, J. Trap-array: A disk arrayarchitecture providing timely recovery to any point-in-time. In

ISCA '06: Proceedings of the 33rd annual international sympo-sium on Computer Architecture (Washington, DC, USA, 2006),
IEEE Computer Society, pp. 289-301.

Notes

1Remus buffers network and disk output. Other devices, such as
the console or the serial port, are presumed to be used for local admin-istration and therefore would not require buffering. However, nothing

prevents these devices from being buffered as well.2Paravirtual Xen guests contain code specifically for suspend requests that are responsible for cleaning up Xen-related state such asshared memory mappings used by virtual devices. In the case of hardware virtualized (e.g., Windows) VMs, this state is completely encap-sulated by Xen's device model, and these in-guest changes are unnecessary.